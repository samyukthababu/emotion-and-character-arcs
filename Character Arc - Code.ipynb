{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kASe_fN3ubxm"
      },
      "outputs": [],
      "source": [
        "pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install word2number"
      ],
      "metadata": {
        "id": "T-zbLetUxESR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install labMTsimple"
      ],
      "metadata": {
        "id": "MMma92IM82U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install marisa-trie"
      ],
      "metadata": {
        "id": "KndpWYR383Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nameparser"
      ],
      "metadata": {
        "id": "qvtrg4K_xNDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install allennlp"
      ],
      "metadata": {
        "id": "mdufLhHUxPVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install allennlp-models"
      ],
      "metadata": {
        "id": "3pOPJbvfxTlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Statements\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import codecs\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import contractions\n",
        "import itertools\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from scipy.sparse import lil_matrix\n",
        "from labMTsimple.speedy import LabMT\n",
        "from word2number import w2n\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "from nameparser import HumanName\n",
        "import nltk\n",
        "from nltk.corpus import names"
      ],
      "metadata": {
        "id": "L5iiMM5tumZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "sEVEYrc-u4Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "_Myp8INQu6bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting a corpus with male and female names from NLTK\n",
        "male_names_dataset = list(set(names.words('male.txt')))\n",
        "female_names_dataset = list(set(names.words(\"female.txt\")))"
      ],
      "metadata": {
        "id": "GF2tG_unyiwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Function to get the title of the book:"
      ],
      "metadata": {
        "id": "lRtc23WAvIsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_title(book):\n",
        "\n",
        "    # Getting the Title of the book\n",
        "    pattern1 = \"Title:\\s(.*)\"\n",
        "    match = re.search(pattern1, book.read())\n",
        "    title = match.group(1)\n",
        "\n",
        "    return title.strip()"
      ],
      "metadata": {
        "id": "yiUK4M9Zu81O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Function to remove the first and last part of the book (Unnecessary):"
      ],
      "metadata": {
        "id": "pOcUqE7avLKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_end_first(book, filename):\n",
        "\n",
        "    # Removing the first part of the book\n",
        "    pattern2 = \"\\*\\*\\*\\s?START.*?\\s?\\*\\*\\*\"\n",
        "    book_clean = re.sub(\"^(?s:.*?)\" + pattern2, \" \", book.read(), count = 1)\n",
        "\n",
        "    # Removing the last part of the book\n",
        "    pattern3 = \"\\*\\*\\*\\s?END.*?\\s?\\*\\*\\*\"\n",
        "    book_clean = re.sub(pattern3 + \"(?s:.*)\", \" \", book_clean, count = 1)\n",
        "\n",
        "    # Removing [Illustrations: ...] from the books\n",
        "    pattern4 = \"\\[Illustration:(.*)\\]\"\n",
        "    book_clean = re.sub(pattern4, \"\", book_clean)\n",
        "\n",
        "    # Removing the text \"The End\" and others from the book\n",
        "    pattern5 = \"(?i)the end|end of the project gutenberg ebook of(.*)|end of project gutenberg's(.*)\"\n",
        "    book_clean = re.sub(pattern5, \"\", book_clean)\n",
        "\n",
        "    # On Manual inspection, these books had the list of all books in the Wizard of Oz Series\n",
        "    # This Regex will be used to remove that\n",
        "    if filename == \"8. Tik-Tok of Oz.txt\" or filename == \"9. The Scarecrow of Oz.txt\" or filename == \"10. Rinkitink in Oz.txt\" or filename == \"12. The Tin Woodman of Oz.txt\" or filename == \"14. Glinda of Oz.txt\":\n",
        "        pattern6 = \"(The Wonderful Oz Books|THE FAMOUS OZ BOOKS)\"\n",
        "        book_clean = re.sub(pattern6 + \"(?s:.*)\", \" \", book_clean)\n",
        "\n",
        "    return book_clean"
      ],
      "metadata": {
        "id": "s8120VxsvNiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Function to Find all the Chapter Titles in the book:"
      ],
      "metadata": {
        "id": "ImXoOKoNvPvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chapter_titles(book, filename):\n",
        "    # Finding all the chapter names in the book\n",
        "    dictTOC = {}\n",
        "    chapter_title = []\n",
        "\n",
        "    if filename == \"3. Ozma of Oz.txt\" or filename == \"4. Dorothy and the Wizard in Oz.txt\" or filename == \"5. The Road to Oz.txt\" or filename == \"13. The Magic of Oz.txt\":\n",
        "        # Pattern seen in Book 3, 4, 5, 13\n",
        "        pattern7 = re.findall(\"(\\d+)(\\.)(\\s+)(.*)\", book)\n",
        "\n",
        "        for i in range(len(pattern7)):\n",
        "            title = pattern7[i][0] + pattern7[i][1] + pattern7[i][2] + pattern7[i][3]\n",
        "            if len(pattern7[i][0]) <= 2:\n",
        "                dictTOC[pattern7[i][0]] = title.strip()\n",
        "    else:\n",
        "        pattern8 = re.findall(\"(Chapter)(.*)\", book)\n",
        "\n",
        "        for i in range(len(pattern8)):\n",
        "            title = pattern8[i][0] + pattern8[i][1]\n",
        "            key_val = w2n.word_to_num(pattern8[i][1])\n",
        "            dictTOC[key_val] = title.strip()\n",
        "\n",
        "    for key in dictTOC:\n",
        "        number = int(key) - 1\n",
        "        chapter_title.insert(number, dictTOC[key])\n",
        "\n",
        "    return chapter_title"
      ],
      "metadata": {
        "id": "l9toa4opvS0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Getting the text for each chapter in the book:"
      ],
      "metadata": {
        "id": "7ioWamEgvVU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def matching_chapter_titles(chapter_title, book_clean, filename, val):\n",
        "\n",
        "    matches = re.finditer(re.escape(chapter_title), book_clean)\n",
        "    count = 0\n",
        "    temp = list(matches)\n",
        "    length = len(temp)\n",
        "\n",
        "    for i in range(length):\n",
        "        if length == 2:\n",
        "            match_index = temp[1].span()[val]\n",
        "        else:\n",
        "            match_index = temp[0].span()[val]\n",
        "\n",
        "    return match_index"
      ],
      "metadata": {
        "id": "vQ4GAQ0_vXgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chapter_chunck(chapter_titles, book_clean, filename):\n",
        "\n",
        "    chapter_chuncks = []\n",
        "\n",
        "    for i in range(len(chapter_titles)):\n",
        "\n",
        "        # Read till the next chapter title\n",
        "        if i != (len(chapter_titles) - 1):\n",
        "\n",
        "            # Getting the chapter title to start and end with\n",
        "            chapter_title = chapter_titles[i]\n",
        "            next_chapter_title = chapter_titles[i + 1]\n",
        "\n",
        "            # Finding the positions in the text\n",
        "            # This will get the chapter chunck\n",
        "\n",
        "            # Start position\n",
        "            end_of_start_chapter_title_index = matching_chapter_titles(chapter_title, book_clean, filename, 1)\n",
        "\n",
        "            # End Position\n",
        "            start_of_next_chapter_title_index = matching_chapter_titles(next_chapter_title, book_clean, filename, 0)\n",
        "\n",
        "            # Getting the chapter chuncks and putting it into a list\n",
        "            chapter_chuncks.append(book_clean[end_of_start_chapter_title_index:start_of_next_chapter_title_index])\n",
        "\n",
        "        # Read till the end\n",
        "        else:\n",
        "            # This is done for the last chapter, where we will read till the end of the book\n",
        "\n",
        "            chapter_title = chapter_titles[i]\n",
        "            end_of_start_chapter_title_index = matching_chapter_titles(chapter_title, book_clean, filename, 1)\n",
        "            chapter_chuncks.append(book_clean[end_of_start_chapter_title_index:])\n",
        "\n",
        "    return chapter_chuncks"
      ],
      "metadata": {
        "id": "xg66YNmYvdPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Function to remove paragraph spacing and weird line breaks:"
      ],
      "metadata": {
        "id": "0anz4GZ2vZaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_spacing_line_breaks(chapter_chuncks):\n",
        "\n",
        "    # Each paragraph is separated using \\r\\n\\r\\n\n",
        "    # Each new line is created using \\r\\n\n",
        "    # Both of these will now be replaced using \\s\n",
        "    # Dealing with contractions as well\n",
        "\n",
        "    chapter_chuncks_clean = []\n",
        "\n",
        "    for chapter in chapter_chuncks:\n",
        "        chapter = chapter.strip()\n",
        "        chapter = chapter.replace(\"\\r\\n\", \" \")\n",
        "        chapter = chapter.replace(\"\\r\\n\\r\\n\", \" \")\n",
        "        chapter = contractions.fix(chapter)\n",
        "        chapter_chuncks_clean.append(chapter)\n",
        "\n",
        "    return chapter_chuncks_clean"
      ],
      "metadata": {
        "id": "KE7KncqNvgK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Function to perform Sentence Tokenisation:"
      ],
      "metadata": {
        "id": "BsfZ8iauvmgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_tokenise(chapter_chuncks_clean):\n",
        "    # Using SpaCy to perform sentence Tokenization\n",
        "\n",
        "    sent_tokenizer = spacy.load(\"en_core_web_sm\")\n",
        "    all_sents_in_each_chapter = []\n",
        "\n",
        "    for chapter in chapter_chuncks_clean:\n",
        "        sent_chapter = []\n",
        "        sentences = sent_tokenizer(chapter)\n",
        "\n",
        "        for sent in sentences.sents:\n",
        "            sent_chapter.append(sent)\n",
        "\n",
        "        all_sents_in_each_chapter.append(sent_chapter)\n",
        "\n",
        "    return all_sents_in_each_chapter"
      ],
      "metadata": {
        "id": "TXSbOyJ-viCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Function to perform Word Tokenisation:"
      ],
      "metadata": {
        "id": "PQrd3QizvpmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_tokenise(chapter_chuncks_clean):\n",
        "    # Using SpaCy to perform word Tokenization\n",
        "    # Loading the English Tokenizer\n",
        "\n",
        "    word_tokenizer = English()\n",
        "    all_words_in_each_chapter = []\n",
        "\n",
        "    for chapter in chapter_chuncks_clean:\n",
        "        words_chapter = []\n",
        "        words = word_tokenizer(chapter.strip())\n",
        "        for word in words:\n",
        "            if not word.is_punct and len(word) != 0 and word.text.strip() != \"\":\n",
        "                words_chapter.append(word.text.lower())\n",
        "        all_words_in_each_chapter.append(words_chapter)\n",
        "\n",
        "    return all_words_in_each_chapter"
      ],
      "metadata": {
        "id": "Re3Nq9rXvqTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Named Entity Recognition (NER) using AllenNLP:"
      ],
      "metadata": {
        "id": "Jx8H2rjTiJ0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NER model\n",
        "ner_model = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/ner-elmo.2021-02-12.tar.gz\")"
      ],
      "metadata": {
        "id": "U-oDp9Dke_pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NER(all_sents_list, title):\n",
        "\n",
        "  PER_tag = []\n",
        "  flag = 0\n",
        "  sentence_id = 0\n",
        "  ner_path = \"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/\" + title + \" - NER.csv\"\n",
        "\n",
        "  for sent in all_sents_list:\n",
        "\n",
        "    person = \"\"\n",
        "    sentence_id += 1\n",
        "    print(sent)\n",
        "\n",
        "    if sentence_id >= 1829:\n",
        "\n",
        "      outcome = ner_model.predict(str(sent))\n",
        "\n",
        "      for tag, word in zip(outcome[\"tags\"], outcome[\"words\"]):\n",
        "        if tag == \"B-PER\":\n",
        "          row = []\n",
        "          person = word\n",
        "          flag = 1\n",
        "          continue\n",
        "\n",
        "        elif tag == \"U-PER\":\n",
        "          row = []\n",
        "          person = word\n",
        "\n",
        "          row_to_append = [person, sentence_id]\n",
        "          with open(ner_path, mode='a', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(row_to_append)\n",
        "          PER_tag.append(row_to_append)\n",
        "\n",
        "        while flag == 1:\n",
        "          if tag == \"I-PER\":\n",
        "            person = person + \" \" + word\n",
        "            break\n",
        "          elif tag == \"L-PER\":\n",
        "            person = person + \" \" + word\n",
        "            flag = 0\n",
        "\n",
        "            row_to_append = [person, sentence_id]\n",
        "            with open(ner_path, mode='a', newline='') as file:\n",
        "              writer = csv.writer(file)\n",
        "              writer.writerow(row_to_append)\n",
        "            PER_tag.append(row_to_append)\n",
        "\n",
        "          else:\n",
        "            flag = 0\n",
        "\n",
        "            row_to_append = [person, sentence_id]\n",
        "            with open(ner_path, mode='a', newline='') as file:\n",
        "              writer = csv.writer(file)\n",
        "              writer.writerow(row_to_append)\n",
        "            PER_tag.append(row_to_append)\n",
        "\n",
        "  return PER_tag"
      ],
      "metadata": {
        "id": "2n-DG5urfEty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the Named Entity Recognition (NER) information from the CSV file\n",
        "def NER_read_csv(title):\n",
        "\n",
        "  ner_path = \"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Named Entities/\" + title + \" - NER.csv\"\n",
        "\n",
        "  dataFrame = pd.read_csv(ner_path, skiprows = [1])\n",
        "  dataFrame = dataFrame.iloc[:, :2]\n",
        "  list_of_names = []\n",
        "\n",
        "  for index, row in dataFrame.iterrows():\n",
        "    row = row.to_dict()\n",
        "    # Append the dictionary to the list\n",
        "    list_of_names.append(row)\n",
        "\n",
        "  return list_of_names"
      ],
      "metadata": {
        "id": "uO5xZGVNiUd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Function to get information related to names such as title, firstname, middlename, lastname, suffix and nickname (Done using NameParser):"
      ],
      "metadata": {
        "id": "SvMpp1dQqtVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_name_info(list_of_names):\n",
        "\n",
        "  name_info = []\n",
        "\n",
        "  for element in list_of_names:\n",
        "    name = HumanName(element[\"name\"])\n",
        "    name = name.as_dict()\n",
        "    name_info.append(name)\n",
        "\n",
        "  return name_info"
      ],
      "metadata": {
        "id": "DdmZFMquqbth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Getting the characters in the book:"
      ],
      "metadata": {
        "id": "FeM959TyzVZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_characters(list_of_names):\n",
        "\n",
        "  characters = []\n",
        "\n",
        "  for name in list_of_names:\n",
        "    characters.append(name[\"name\"])\n",
        "\n",
        "  characters = list(set(characters))\n",
        "\n",
        "  return characters"
      ],
      "metadata": {
        "id": "B8N15rAezTjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Corrections to some of the Character names:"
      ],
      "metadata": {
        "id": "VOs0y7cM2XaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correction_function(list_of_names, title):\n",
        "\n",
        "  list_of_names_corrected = list_of_names[:]\n",
        "\n",
        "  for name in list_of_names_corrected:\n",
        "\n",
        "    if title == \"Ozma of Oz\":\n",
        "\n",
        "      if name[\"name\"] == \"Jin\":\n",
        "        name[\"name\"] = \"Jin-jur\"\n",
        "\n",
        "      elif name[\"name\"] == \"Nome\":\n",
        "        name[\"name\"] = \"Nome King\"\n",
        "\n",
        "      elif name[\"name\"] == \"ma\":\n",
        "        name[\"name\"] = \"Oz-ma\"\n",
        "\n",
        "    elif title == \"Dorothy and the Wizard in Oz\":\n",
        "\n",
        "      if name[\"name\"] == \"Bug\" or name[\"name\"] == \"Woggle\" or name[\"name\"] == \"T.E.\":\n",
        "        name[\"name\"] = \"Woggle-Bug\"\n",
        "\n",
        "      elif name[\"name\"] == \"Anu\":\n",
        "        name[\"name\"] = \"Overman-Anu\"\n",
        "\n",
        "      elif name[\"name\"] == \"Jim the Cab\":\n",
        "        name[\"name\"] = \"Jim the Cab-horse\"\n",
        "\n",
        "      elif name[\"name\"] == \"Tik\":\n",
        "        name[\"name\"] = \"Tik-tok\"\n",
        "\n",
        "    elif title == \"The Road to Oz\":\n",
        "\n",
        "      if name[\"name\"] == \"Braided\":\n",
        "        name[\"name\"] = \"Braided Man\"\n",
        "\n",
        "      elif name[\"name\"] == \"Kik\":\n",
        "        name[\"name\"] = \"Kik-a-bray\"\n",
        "\n",
        "      elif name[\"name\"] == \"Tok\":\n",
        "        name[\"name\"] = \"Tok-Tok\"\n",
        "\n",
        "      elif name[\"name\"] == \"H. M. Wogglebug\":\n",
        "        name[\"name\"] = \"Wogglebug\"\n",
        "\n",
        "      elif name[\"name\"] == \"Ix\":\n",
        "        name[\"name\"] = \"Queen Zixi of Ix\"\n",
        "\n",
        "      elif name[\"name\"] == \"Tik\":\n",
        "        name[\"name\"] = \"Tik-tok\"\n",
        "\n",
        "      elif name[\"name\"] == \"Bug\" or name[\"name\"] == \"H. M.\":\n",
        "        name[\"name\"] = \"Woggle-Bug\"\n",
        "\n",
        "      elif name[\"name\"] == \"Bright\":\n",
        "        name[\"name\"] = \"Button-Bright\"\n",
        "\n",
        "      elif name[\"name\"] == \"Nome\":\n",
        "        name[\"name\"] = \"Nome King\"\n",
        "\n",
        "    elif title == \"The Emerald City of Oz\":\n",
        "\n",
        "      if name[\"name\"] == \"Scarecrow Ozma\":\n",
        "        name[\"name\"] = \"Scarecrow\"\n",
        "\n",
        "      elif name[\"name\"] == \"H. M. Wogglebug\":\n",
        "        name[\"name\"] = \"Wogglebug\"\n",
        "\n",
        "      elif name[\"name\"] == \"Over\":\n",
        "        name[\"name\"] = \"Mr. Over\"\n",
        "\n",
        "      elif name[\"name\"] == \"Uncle Henry--\":\n",
        "        name[\"name\"] = \"Uncle Henry\"\n",
        "\n",
        "      elif name[\"name\"] == \"Aunt\" or name[\"name\"] == \"Them\":\n",
        "        name[\"name\"] = \"Aunt Them\"\n",
        "\n",
        "    elif title == \"The Patchwork Girl of Oz\":\n",
        "\n",
        "      if name[\"name\"] == \"Tottenhot Waits\":\n",
        "        name[\"name\"] = \"Tottenhot\"\n",
        "\n",
        "      elif name[\"name\"] == \"Woggle\":\n",
        "        name[\"name\"] = \"Woggle-Bug\"\n",
        "\n",
        "      elif name[\"name\"] == \"Tok\":\n",
        "        name[\"name\"] = \"Tik-Tok\"\n",
        "\n",
        "      elif name[\"name\"] == \"Kroo\" or name[\"name\"] == \"Krizzle\":\n",
        "        name[\"name\"] = \"Krizzle-Kroo\"\n",
        "\n",
        "      elif name[\"name\"] == \"Ozma'll\":\n",
        "        name[\"name\"] = \"Ozma\"\n",
        "\n",
        "      elif name[\"name\"] == \"Wise Scarecrow\":\n",
        "        name[\"name\"] = \"Scarecrow\"\n",
        "\n",
        "      elif name[\"name\"] == \"I. Toto\":\n",
        "        name[\"name\"] = \"Toto\"\n",
        "\n",
        "      elif name[\"name\"] == \"Journey Ojo\":\n",
        "        name[\"name\"] = \"Ojo\"\n",
        "\n",
        "      elif name[\"name\"] == \"Dorothy Dorothy Gale\":\n",
        "        name[\"name\"] = \"Dorothy Gale\"\n",
        "\n",
        "      elif name[\"name\"] == \"een Ojo\":\n",
        "        name[\"name\"] = \"Ojo\"\n",
        "\n",
        "    elif title == \"Tik-Tok of Oz\":\n",
        "\n",
        "      if name[\"name\"] == \"Tititi\":\n",
        "        name[\"name\"] = \"Tititi-Hoocho\"\n",
        "\n",
        "      elif name[\"name\"] == \"Eared Hearer\":\n",
        "        name[\"name\"] = \"Long-Eared Hearer\"\n",
        "\n",
        "      elif name[\"name\"] == \"Tok\":\n",
        "        name[\"name\"] = \"Tik-Tok\"\n",
        "\n",
        "      elif name[\"name\"] == \"Nome\":\n",
        "        name[\"name\"] = \"Nome King\"\n",
        "\n",
        "      elif name[\"name\"] == \"Naughty Nome Shaggy Man\":\n",
        "        name[\"name\"] = \"Shaggy Man\"\n",
        "\n",
        "    elif title == \"The Scarecrow of Oz\":\n",
        "\n",
        "      if name[\"name\"] == \"Trot!—is\":\n",
        "        name[\"name\"] = \"Trot\"\n",
        "\n",
        "      elif name[\"name\"] == \"Tok\":\n",
        "        name[\"name\"] = \"Tik-Tok\"\n",
        "\n",
        "      elif name[\"name\"] == \"Googly\":\n",
        "        name[\"name\"] = \"Googly-Goo\"\n",
        "\n",
        "      elif name[\"name\"] == \"Bumpy\":\n",
        "        name[\"name\"] = \"Bumpy Man\"\n",
        "\n",
        "      elif name[\"name\"] == \"Hopper\":\n",
        "        name[\"name\"] = \"Grass-Hopper\"\n",
        "\n",
        "      elif name[\"name\"] == \"Button\":\n",
        "        name[\"name\"] = \"Button-Bright\"\n",
        "\n",
        "    elif title == \"Rinkitink in Oz\":\n",
        "\n",
        "      if name[\"name\"] == \"Zella Goes\" or name[\"name\"] == \"Zella Saves\":\n",
        "        name[\"name\"] = \"Zella\"\n",
        "\n",
        "      elif name[\"name\"] == \"Kitticut?--a\":\n",
        "        name[\"name\"] = \"Kitticut\"\n",
        "\n",
        "      elif name[\"name\"] == \"Said Rinkitink\":\n",
        "        name[\"name\"] = \"Rinkitink\"\n",
        "\n",
        "      elif name[\"name\"] == \"Inga Parts\":\n",
        "        name[\"name\"] = \"Inga\"\n",
        "\n",
        "      elif name[\"name\"] == \"Wooden Sawhorse\":\n",
        "        name[\"name\"] = \"Sawhorse\"\n",
        "\n",
        "      elif name[\"name\"] == \"Nome\":\n",
        "        name[\"name\"] = \"Nome King\"\n",
        "\n",
        "      elif name[\"name\"] == \"Tiny Trot\":\n",
        "        name[\"name\"] = \"Trot\"\n",
        "\n",
        "    elif title == \"The Tin Woodman of Oz\":\n",
        "\n",
        "      if name[\"name\"] == \"Klip\":\n",
        "        name[\"name\"] = \"Ku-Klip\"\n",
        "\n",
        "      elif name[\"name\"] == \"Yookoohoo Woot\":\n",
        "        name[\"name\"] = \"Woot\"\n",
        "\n",
        "      elif name[\"name\"] == \"Brown\":\n",
        "        name[\"name\"] = \"Brown Bear\"\n",
        "\n",
        "      elif name[\"name\"] == \"Tik\":\n",
        "        name[\"name\"] = \"Tik-tok\"\n",
        "\n",
        "      elif name[\"name\"] == \"Wooden Sawhorse\":\n",
        "        name[\"name\"] = \"Sawhorse\"\n",
        "\n",
        "      elif name[\"name\"] == \"Bright\":\n",
        "        name[\"name\"] = \"Button-Bright\"\n",
        "\n",
        "      elif name[\"name\"] == \"Tiny Trot\":\n",
        "        name[\"name\"] = \"Trot\"\n",
        "\n",
        "    elif title == \"The Magic of Oz\":\n",
        "\n",
        "      if name[\"name\"] == \"Bright\":\n",
        "        name[\"name\"] = \"Button-Bright\"\n",
        "\n",
        "      elif name[\"name\"] == \"Lucky\":\n",
        "        name[\"name\"] = \"Ojo the Lucky\"\n",
        "\n",
        "      elif name[\"name\"] == \"Tik\":\n",
        "        name[\"name\"] = \"Tik-Tok\"\n",
        "\n",
        "      elif name[\"name\"] == \"O Rango\":\n",
        "        name[\"name\"] = \"Rango the Gray Ape\"\n",
        "\n",
        "      elif name[\"name\"] == \"OZMA\" or name[\"name\"] == \"Ozma Has\":\n",
        "        name[\"name\"] = \"Ozma\"\n",
        "\n",
        "      elif name[\"name\"] == \"H. M. Wogglebug\":\n",
        "        name[\"name\"] = \"Wogglebug\"\n",
        "\n",
        "      elif name[\"name\"] == \"Wooden Sawhorse\":\n",
        "        name[\"name\"] = \"Sawhorse\"\n",
        "\n",
        "      elif name[\"name\"] == \"Eag\":\n",
        "        name[\"name\"] = \"Li-Mon-Eag\"\n",
        "\n",
        "    elif title == \"Glinda of Oz\":\n",
        "\n",
        "      if name[\"name\"] == \"Su\":\n",
        "        name[\"name\"] = \"Su-dic\"\n",
        "\n",
        "      elif name[\"name\"] == \"Coo\":\n",
        "        name[\"name\"] = \"Coo-ee-oh\"\n",
        "\n",
        "      elif name[\"name\"] == \"Great Sorceress Betsy\":\n",
        "        name[\"name\"] = \"Betsy\"\n",
        "\n",
        "      elif name[\"name\"] == \"Lucky\":\n",
        "        name[\"name\"] = \"Ojo the Lucky\"\n",
        "\n",
        "      elif name[\"name\"] == \"Magic Isle Ozma\":\n",
        "        name[\"name\"] = \"Ozma\"\n",
        "\n",
        "      elif name[\"name\"] == \"Tok\":\n",
        "        name[\"name\"] = \"Tik-Tok\"\n",
        "\n",
        "      elif name[\"name\"] == \"H. M. Wogglebug\":\n",
        "        name[\"name\"] = \"Wogglebug\"\n",
        "\n",
        "      elif name[\"name\"] == \"Nome\":\n",
        "        name[\"name\"] = \"Nome King\"\n",
        "\n",
        "      elif name[\"name\"] == \"Flathead Su\":\n",
        "        name[\"name\"] = \"Flathead Su-dic\"\n",
        "\n",
        "      elif name[\"name\"] == \"Betsy Glinda\":\n",
        "        name[\"name\"] = \"Glinda\"\n",
        "\n",
        "  return list_of_names_corrected"
      ],
      "metadata": {
        "id": "tSdnPkJw2Ubo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Function to get the gender of the characters:"
      ],
      "metadata": {
        "id": "Wxn5eTpV62di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation\n",
        "male_titles = [\"Uncle\", \"Mr\", \"Prince\", \"King\", \"Emperor\", \"Colonel\", \"Judge\", \"Mister\", \"Dr\", \"Professor\", \"Brother\", \"Private\",\n",
        "               \"General\", \"Captain\", \"Chief\", \"Wizard\", \"Mr.\", \"Royal\", \"Emperor\"]\n",
        "female_titles = [\"Mother\", \"Princess\", \"Queen\", \"Aunt\", \"Miss\", \"Empress\", \"Dame\", \"Mrs\", \"Lady\", \"Witch\", \"Madam\", \"Highness\"]\n",
        "female_pronouns = [\"she\", \"her\", \"hers\", \"herself\", \"Princess\", \"Queen\", \"niece\", \"girl\"]\n",
        "male_pronouns = [\"he\", \"him\", \"himself\", \"Prince\", \"King\", \"nephew\", \"boy\"]"
      ],
      "metadata": {
        "id": "THlVFPXY6oZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the text file to get additional male names\n",
        "with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Female names.txt\", \"r\") as f:\n",
        "    female_name = f.readlines()\n",
        "female_name = [name.strip() for name in female_name]"
      ],
      "metadata": {
        "id": "JTUlZlXY65bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the text file to get additional female names\n",
        "with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Male names.txt\", \"r\") as f:\n",
        "     male_name = f.readlines()\n",
        "male_name = [name.strip() for name in male_name]"
      ],
      "metadata": {
        "id": "3-Oop8ZB75_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gender_identification(total_names_info, title):\n",
        "\n",
        "  detected_name = []\n",
        "\n",
        "  for i in range(len(total_names_info)):\n",
        "\n",
        "    gender = \"\"\n",
        "\n",
        "    # The Title provides useful information for gender identification\n",
        "    if len(total_names_info[i][\"title\"]) > 0:\n",
        "      # Check to see if they are in the list of male or female titles\n",
        "      if total_names_info[i][\"title\"] in male_titles:\n",
        "        gender = \"male\"\n",
        "\n",
        "      elif total_names_info[i][\"title\"] in female_titles:\n",
        "        gender = \"female\"\n",
        "\n",
        "      total_names_info[i][\"gender\"] = gender\n",
        "\n",
        "      # All the other character names with the same will be updated as having the same gender\n",
        "      for j in range(i + 1, len(total_names_info)):\n",
        "        if total_names_info[i][\"name\"] == total_names_info[j][\"name\"]:\n",
        "          total_names_info[j][\"gender\"] = gender\n",
        "\n",
        "    # The Title does not provide any useful information\n",
        "    # Using the First name for gender idenfication\n",
        "    elif len(total_names_info[i][\"title\"]) == 0 and len(total_names_info[i][\"gender\"]) == 0 and total_names_info[i][\"first\"] in male_names_dataset or total_names_info[i][\"first\"] in female_names_dataset or total_names_info[i][\"first\"] in male_name or total_names_info[i][\"first\"] in female_name:\n",
        "\n",
        "      # Check to see if the names are in the dataset\n",
        "      if total_names_info[i][\"first\"] in female_name:\n",
        "        gender = \"female\"\n",
        "      elif total_names_info[i][\"first\"] in male_name:\n",
        "        gender = \"male\"\n",
        "      elif total_names_info[i][\"first\"] in male_names_dataset:\n",
        "        gender = \"male\"\n",
        "      elif total_names_info[i][\"first\"] in female_names_dataset:\n",
        "        gender = \"female\"\n",
        "\n",
        "      total_names_info[i][\"gender\"] = gender\n",
        "\n",
        "      # All the other character names with the same will be updated as having the same gender\n",
        "      for j in range(i + 1, len(total_names_info)):\n",
        "        if total_names_info[i][\"name\"] == total_names_info[j][\"name\"]:\n",
        "          total_names_info[j][\"gender\"] = gender\n",
        "\n",
        "  for k in range(len(total_names_info)):\n",
        "\n",
        "    # The Title does not provide any useful information\n",
        "    # The First name does not provide any useful information\n",
        "    # Using the context of the sentence to get the information\n",
        "    if len(total_names_info[k][\"gender\"]) == 0:\n",
        "\n",
        "      name = total_names_info[k][\"name\"]\n",
        "      detected_name.append(name)\n",
        "\n",
        "  for d_name in detected_name:\n",
        "    print(d_name)\n",
        "\n",
        "  unknown_names_gender_context(detected_name, total_names_info)\n",
        "\n",
        "  return total_names_info"
      ],
      "metadata": {
        "id": "Iy5f7vIT77xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unknown_names_gender_context(detected_name, total_names_info):\n",
        "\n",
        "  nlp = English()\n",
        "  word_tokenizer = nlp.tokenizer\n",
        "  detected_name = list(set(detected_name))\n",
        "\n",
        "  for i in range(len(detected_name)):\n",
        "    if \"-\" in detected_name[i]:\n",
        "      detected_name[i] = re.sub(\"-\\s\", \"\", detected_name[i])\n",
        "\n",
        "  for i in range(len(total_names_info)):\n",
        "\n",
        "    if len(total_names_info[i][\"gender\"]) == 0:\n",
        "\n",
        "      # Sentence associated with the character\n",
        "      tokenised_sent = word_tokenizer(str(all_sents_list[(total_names_info[i][\"sentence_id\"]) - 1]))\n",
        "\n",
        "      # Word Tokenisation\n",
        "      words = []\n",
        "      for word in tokenised_sent:\n",
        "        if not word.is_punct and len(word) != 0 and word.text.strip() != \"\":\n",
        "          words.append(word.text)\n",
        "\n",
        "      # If the name has more then one word, then we will find the index of the last word in the name\n",
        "      # We will get the 5 words after that\n",
        "      if total_names_info[i][\"name\"].count(\" \") >= 1:\n",
        "\n",
        "        name_split = total_names_info[i][\"name\"].split()\n",
        "        length = len(name_split)\n",
        "        name = name_split[length - 1]\n",
        "      else:\n",
        "\n",
        "        name = total_names_info[i][\"name\"]\n",
        "\n",
        "      try:\n",
        "        # Initialisation\n",
        "        index = words.index(name)\n",
        "        female_pronouns_counter = 0\n",
        "        male_pronouns_counter = 0\n",
        "        right_three_words = []\n",
        "        left_three_words = []\n",
        "\n",
        "        # Getting the context (5 words after the occurence of the name)\n",
        "        if len(words) - (index + 1) == 1 and len(words) - (index + 1) == 2 and len(words) - (index + 1) == 3 and len(words) - (index + 1) == 4 and len(words) - (index + 1) == 5:\n",
        "        # Read till the end\n",
        "          right_three_words = words[index + 1:]\n",
        "        else:\n",
        "        # Read the next 5 words after the occurence of a character\n",
        "          right_three_words = words[index + 1:index + 6]\n",
        "\n",
        "        # Getting the context (5 words before the occurence of the name)\n",
        "        if index == 0:\n",
        "          pass\n",
        "        elif index == 1 or index == 2 or index == 3 or index == 4 or index == 5:\n",
        "          left_three_words = words[:index]\n",
        "        else:\n",
        "          left_three_words = words[index - 5:index]\n",
        "\n",
        "        # Putting the context into a single list\n",
        "        trigram = left_three_words + right_three_words\n",
        "\n",
        "        # Counter to check whether the male or female pronouns are present\n",
        "        for t in trigram:\n",
        "          if t in female_pronouns:\n",
        "            female_pronouns_counter += 1\n",
        "          elif t in male_pronouns:\n",
        "            male_pronouns_counter += 1\n",
        "\n",
        "        # Whichever pronoun has a higher count, the character will be assigned that gender\n",
        "        if female_pronouns_counter > male_pronouns_counter:\n",
        "          gender = \"female\"\n",
        "        elif male_pronouns_counter > female_pronouns_counter:\n",
        "          gender = \"male\"\n",
        "        else:\n",
        "          gender = \"unknown\"\n",
        "\n",
        "        if gender == \"unknown\":\n",
        "          total_names_info[i][\"gender\"] = gender\n",
        "\n",
        "        else:\n",
        "          total_names_info[i][\"gender\"] = gender\n",
        "\n",
        "          # All the other character names with the same will be updated as having the same gender\n",
        "          for j in range(i + 1, len(total_names_info)):\n",
        "            if total_names_info[i][\"name\"] == total_names_info[j][\"name\"]:\n",
        "              total_names_info[j][\"gender\"] = gender\n",
        "\n",
        "      except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "QY42Jtt_8w7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Function to store the characters and all the related information:"
      ],
      "metadata": {
        "id": "mQ9sKqXsCEKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_all_info(total_names_info, title):\n",
        "  # Inserting all the information related to the characters into a CSV file for each book\n",
        "  # Headings\n",
        "  headings = [\"name\", \"sentence_id\", \"title\", \"first\", \"middle\", \"last\", \"suffix\", \"nickname\", \"gender\"]\n",
        "\n",
        "  # Inserting character information into the csv files\n",
        "  for i in range(len(total_names_info)):\n",
        "    row_to_insert = []\n",
        "\n",
        "    row_to_insert.append(total_names_info[i][\"name\"])\n",
        "    row_to_insert.append(total_names_info[i][\"sentence_id\"])\n",
        "\n",
        "    for j in range(2, len(headings)):\n",
        "      temp = headings[j]\n",
        "      if len(total_names_info[i][temp]) > 0:\n",
        "        row_to_insert.append(total_names_info[i][temp])\n",
        "      else:\n",
        "        row_to_insert.append(\"none\")\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Characters Information/\" + title + \" - All Info.csv\", \"a\", newline = \"\") as f:\n",
        "        write = csv.writer(f)\n",
        "        write.writerow(row_to_insert)"
      ],
      "metadata": {
        "id": "EHrcs2gjCATr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Matching mulitple entities and their nicknames:"
      ],
      "metadata": {
        "id": "DT2E1Ou64v0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def matching_algorithm(title):\n",
        "\n",
        "  # Reading the CSV file and getting the list of characters\n",
        "  dataFrame = pd.read_csv(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Characters Information/\" + title + \" - All Info.csv\")\n",
        "  character_names = dataFrame[\"name\"].tolist()\n",
        "  character_names = list(set(character_names))\n",
        "\n",
        "  # Converting the DataFrame into a list\n",
        "  total_info = dataFrame.to_dict(orient = \"list\")\n",
        "\n",
        "  # Matching Algorithm\n",
        "  all_entities = []\n",
        "\n",
        "  for i in range(len(total_info[\"name\"])):\n",
        "\n",
        "    entity = []\n",
        "\n",
        "    # Check to see if there exists names with title, first name and last name\n",
        "    # This is the most complete form of the name\n",
        "    if total_info[\"title\"][i] != \"none\" and total_info[\"first\"][i] != \"none\" and total_info[\"last\"][i] != \"none\":\n",
        "      name = total_info[\"name\"][i]\n",
        "\n",
        "      if len(all_entities) != 0:\n",
        "        # Check to see if that name already exists\n",
        "        if any(name in temp for temp in all_entities) or name in entity:\n",
        "          continue\n",
        "        else:\n",
        "          entity.append(name)\n",
        "\n",
        "      else:\n",
        "        if name in entity:\n",
        "          continue\n",
        "        else:\n",
        "          entity.append(name)\n",
        "\n",
        "      # Looking for matches\n",
        "      for j in range(i + 1, len(total_info[\"name\"])):\n",
        "\n",
        "        # Look for first and last names that match\n",
        "        if total_info[\"first\"][j] == total_info[\"first\"][i] and total_info[\"last\"][j] == total_info[\"last\"][i] and total_info[\"gender\"][j] == total_info[\"gender\"][i]:\n",
        "\n",
        "          name = total_info[\"name\"][j]\n",
        "\n",
        "          if len(all_entities) != 0:\n",
        "            # Check to see if that name already exists\n",
        "            if any(name in temp for temp in all_entities) or name in entity:\n",
        "              continue\n",
        "            else:\n",
        "              entity.append(name)\n",
        "\n",
        "          else:\n",
        "            if name in entity:\n",
        "              continue\n",
        "            else:\n",
        "              entity.append(name)\n",
        "\n",
        "        # Look for first names that match\n",
        "        elif total_info[\"first\"][j] == total_info[\"first\"][i] and total_info[\"gender\"][j] == total_info[\"gender\"][i]:\n",
        "\n",
        "          name = total_info[\"name\"][j]\n",
        "\n",
        "          if len(all_entities) != 0:\n",
        "            # Check to see if that name already exists\n",
        "            if any(name in temp for temp in all_entities) or name in entity:\n",
        "              continue\n",
        "            else:\n",
        "              entity.append(name)\n",
        "\n",
        "          else:\n",
        "            if name in entity:\n",
        "              continue\n",
        "            else:\n",
        "              entity.append(name)\n",
        "\n",
        "      all_entities.append(entity)\n",
        "\n",
        "    # Check to see if there exists first and last name\n",
        "    # This is the second most complete form of the name\n",
        "    if total_info[\"first\"][i] != \"none\" and total_info[\"last\"][i] != \"none\":\n",
        "      name = total_info[\"name\"][i]\n",
        "\n",
        "      if len(all_entities) != 0:\n",
        "        # Check to see if that name already exists\n",
        "        if any(name in temp for temp in all_entities) or name in entity:\n",
        "          continue\n",
        "        else:\n",
        "          entity.append(name)\n",
        "\n",
        "      else:\n",
        "        if name in entity:\n",
        "          continue\n",
        "        else:\n",
        "          entity.append(name)\n",
        "\n",
        "      # Looking for matches\n",
        "      for j in range(i + 1, len(total_info[\"name\"])):\n",
        "        # Look for first names that match\n",
        "        if total_info[\"first\"][j] == total_info[\"first\"][i] and total_info[\"gender\"][j] == total_info[\"gender\"][i]:\n",
        "\n",
        "          name = total_info[\"name\"][j]\n",
        "\n",
        "          if len(all_entities) != 0:\n",
        "            # Check to see if that name already exists\n",
        "            if any(name in temp for temp in all_entities) or name in entity:\n",
        "              continue\n",
        "            else:\n",
        "              entity.append(name)\n",
        "\n",
        "          else:\n",
        "            if name in entity:\n",
        "              continue\n",
        "            else:\n",
        "              entity.append(name)\n",
        "\n",
        "      all_entities.append(entity)\n",
        "\n",
        "    # Only names with first names\n",
        "    if total_info[\"first\"][i] != \"none\":\n",
        "      name = total_info[\"name\"][i]\n",
        "\n",
        "      if len(all_entities) != 0:\n",
        "        # Check to see if that name already exists\n",
        "        if any(name in temp for temp in all_entities) or name in entity:\n",
        "          continue\n",
        "        else:\n",
        "          entity.append(name)\n",
        "\n",
        "      else:\n",
        "        if name in entity:\n",
        "          continue\n",
        "        else:\n",
        "          entity.append(name)\n",
        "\n",
        "      # Looking for matches\n",
        "      for j in range(i + 1, len(total_info[\"name\"])):\n",
        "        # Look for first names that match\n",
        "        if total_info[\"first\"][j] == total_info[\"first\"][i] and total_info[\"gender\"][j] == total_info[\"gender\"][i]:\n",
        "\n",
        "          name = total_info[\"name\"][j]\n",
        "\n",
        "          if len(all_entities) != 0:\n",
        "            # Check to see if that name already exists\n",
        "            if any(name in temp for temp in all_entities) or name in entity:\n",
        "              continue\n",
        "            else:\n",
        "              entity.append(name)\n",
        "\n",
        "          else:\n",
        "            if name in entity:\n",
        "              continue\n",
        "            else:\n",
        "              entity.append(name)\n",
        "\n",
        "      all_entities.append(entity)\n",
        "\n",
        "  # Characters that are found by the matching algorithm\n",
        "  matched_char = [ele for subl in all_entities for ele in subl]\n",
        "\n",
        "  for char in character_names:\n",
        "    if char not in matched_char:\n",
        "      all_entities.append([char])\n",
        "\n",
        "  return all_entities"
      ],
      "metadata": {
        "id": "mSCMmHAqre5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nicknames_and_matches(title, all_entities):\n",
        "\n",
        "  nickname_list = []\n",
        "  new_list = []\n",
        "  final_entities = []\n",
        "\n",
        "  # Reading the nicknames file\n",
        "  with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Nicknames.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "      line = line.strip()\n",
        "      words = line.split(',')\n",
        "      nickname_list.append(words)\n",
        "\n",
        "  # Getting all the nicknames for each of the matches\n",
        "  for entity in all_entities:\n",
        "    for character in entity:\n",
        "      flag = 0\n",
        "      for nicknames in nickname_list:\n",
        "        if character in nicknames:\n",
        "          flag += 1\n",
        "          break\n",
        "\n",
        "    if flag >= 1:\n",
        "      new_list.append(entity + nicknames)\n",
        "    elif flag == 0:\n",
        "      new_list.append(entity)\n",
        "\n",
        "  # Removing the duplicate elements from the inner list\n",
        "  for l in new_list:\n",
        "    final_entities.append(list(set(l)))\n",
        "\n",
        "  # Removing duplicate inner lists\n",
        "  sort_i_list = [frozenset(i_l) for i_l in final_entities]\n",
        "  unique_entities_temp = set(sort_i_list)\n",
        "  unique_entities = [list(i_s) for i_s in unique_entities_temp]\n",
        "\n",
        "  return unique_entities"
      ],
      "metadata": {
        "id": "V5Weovps41zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. Entity Resolution and Top Characters:"
      ],
      "metadata": {
        "id": "p6EYo5VonVaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entity_resolution(title):\n",
        "\n",
        "  # Reading the matched entities text file\n",
        "  matched_entities = []\n",
        "\n",
        "  with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Matched Characters and Nicknames/\" + title + \" - Matches.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "      line = line.strip()\n",
        "      words = line.split(',')\n",
        "      matched_entities.append(words)\n",
        "\n",
        "  # Saving the longest name as the one to replace all the other names\n",
        "  matched_entities_dict = []\n",
        "  for item in matched_entities:\n",
        "\n",
        "    if len(item) != 1:\n",
        "      dictionary = {\"name\":\"\", \"other\": []}\n",
        "      longest_name = max(item, key = len)\n",
        "\n",
        "      dictionary[\"name\"] = longest_name\n",
        "      item.remove(longest_name)\n",
        "\n",
        "      for character in item:\n",
        "        dictionary[\"other\"].append(character)\n",
        "\n",
        "    elif len(item) == 1:\n",
        "      dictionary = {\"name\":item[0], \"other\":[]}\n",
        "\n",
        "    matched_entities_dict.append(dictionary)\n",
        "\n",
        "  # Reading the Characters - All Info CSV file and reading only the names and the sentence_id\n",
        "  dataFrame1 = pd.read_csv(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Characters Information/\" + title + \" - All Info.csv\", skiprows = [1])\n",
        "  dataFrame1 = dataFrame1.iloc[:, :2]\n",
        "  list_of_names1 = []\n",
        "\n",
        "  for index, row in dataFrame1.iterrows():\n",
        "    row = row.to_dict()\n",
        "    # Append the dictionary to the list\n",
        "    list_of_names1.append(row)\n",
        "\n",
        "  # Creating a CSV file with the characters and the resolved names\n",
        "  for ele in list_of_names1:\n",
        "\n",
        "    name = ele[\"name\"]\n",
        "    sentence_id = ele[\"sentence_id\"]\n",
        "    new_name = \"\"\n",
        "    row_to_insert = []\n",
        "\n",
        "    for entity in matched_entities_dict:\n",
        "      if len(entity[\"other\"]) == 0:\n",
        "        if name in entity[\"name\"]:\n",
        "          new_name = entity[\"name\"]\n",
        "      else:\n",
        "        if name in entity[\"other\"]:\n",
        "          new_name = entity[\"name\"]\n",
        "        elif name == entity[\"name\"]:\n",
        "          new_name = name\n",
        "\n",
        "    row_to_insert.append(name)\n",
        "    row_to_insert.append(sentence_id)\n",
        "    row_to_insert.append(new_name)\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Entity Changes/\" + title + \" - Entity Changes.csv\", \"a\", newline = \"\") as f:\n",
        "      write = csv.writer(f)\n",
        "      write.writerow(row_to_insert)\n"
      ],
      "metadata": {
        "id": "h4DloeFunT-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_10_characters(title):\n",
        "\n",
        "    # Getting the top 10 characters based on their occurences\n",
        "    cols = [\"name\", \"sentence_id\", \"new_name\"]\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Entity Changes/\" + title + \" - Entity Changes.csv\", usecols = cols)\n",
        "    unique_values = df[\"new_name\"].value_counts()\n",
        "    top_10_characters = unique_values.nlargest(10)\n",
        "    top_characters = [(character, num_occurences) for character, num_occurences in zip(top_10_characters.index, top_10_characters.values)]\n",
        "\n",
        "    # Saving this information in a text file\n",
        "    with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Top 10 characters/\" + title + \" - Top 10 Characters.txt\", \"w\", newline = \"\") as f:\n",
        "      csv_writer = csv.writer(f)\n",
        "      csv_writer.writerows(top_characters)"
      ],
      "metadata": {
        "id": "P1kosZM5rKD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resolution(title, all_sents_list):\n",
        "\n",
        "  # Reading the entity changes file\n",
        "  dataFrame1 = pd.read_csv(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Entity Changes/\" + title + \" - Entity Changes.csv\")\n",
        "  dataFrame1 = dataFrame1.iloc[:, :3]\n",
        "  list_of_names1 = []\n",
        "  entity_changes_sents = all_sents_list.copy()\n",
        "\n",
        "  for index, row in dataFrame1.iterrows():\n",
        "    row = row.to_dict()\n",
        "    # Append the dictionary to the list\n",
        "    list_of_names1.append(row)\n",
        "\n",
        "  # Resolving the entities into a single entity\n",
        "  for item in list_of_names1:\n",
        "    name = item[\"name\"]\n",
        "    sentence_index = item[\"sentence_id\"] - 1\n",
        "    new_name = item[\"new_name\"]\n",
        "    sentence = str(entity_changes_sents[sentence_index])\n",
        "    entity_changes_sents[sentence_index] = sentence.replace(name, new_name)\n",
        "\n",
        "  # Saving the original list of sentences to a text file\n",
        "  with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Original Text - Sentences/\" + title + \" - Original.txt\", 'w') as f:\n",
        "    for sentence in all_sents_list:\n",
        "      f.write(str(sentence) + '\\n')\n",
        "\n",
        "  # Saving the changed sentences to a text file\n",
        "  with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/New text - After Character Resolution/\" + title + \" - Resolved.txt\", 'w') as f:\n",
        "    for sentence in entity_changes_sents:\n",
        "      f.write(str(sentence) + '\\n')\n"
      ],
      "metadata": {
        "id": "GqP9ffze9mby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. Coreference Resolution:"
      ],
      "metadata": {
        "id": "S72lqA_3jr7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model\n",
        "model_url = \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\"\n",
        "predictor = Predictor.from_path(model_url)"
      ],
      "metadata": {
        "id": "chjr2qBhjuzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def coreference_resolution(title):\n",
        "\n",
        "  text_coref = []\n",
        "  coreference_resolved_text = []\n",
        "  window_size = 5\n",
        "  sent_tokenizer = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "  # Reading the respective file\n",
        "  with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/New text - After Character Resolution/\" + title + \" - Resolved.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "      text_coref.append(line.strip())\n",
        "\n",
        "  for i in range(0, len(text_coref), 5):\n",
        "    start_index = i\n",
        "    end_index = i + window_size\n",
        "\n",
        "    if end_index > len(text_coref):\n",
        "      text = ' '.join(text_coref[start_index:])\n",
        "    else:\n",
        "      text = ' '.join(text_coref[start_index:end_index])\n",
        "\n",
        "    coref_text = predictor.coref_resolved(text)\n",
        "\n",
        "    # Getting back the sentences\n",
        "    sentences = sent_tokenizer(coref_text)\n",
        "    for sent in sentences.sents:\n",
        "      coreference_resolved_text.append(sent)\n",
        "\n",
        "  # Saving the information\n",
        "  with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/New text - Coreference Resolution/\" + title + \" - Coref.txt\", 'w') as f:\n",
        "    for sentence in coreference_resolved_text:\n",
        "      f.write(str(sentence) + '\\n')"
      ],
      "metadata": {
        "id": "BhJjQFsTjxQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. Semantic Role Labelling:"
      ],
      "metadata": {
        "id": "zPfkkOKbqC0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "srl_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")"
      ],
      "metadata": {
        "id": "L3uTf1ycqHeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is for the Coference Resolution Text\n",
        "\n",
        "def semantic_role_labelling_coref(title):\n",
        "\n",
        "  coreference_resolved_text = []\n",
        "\n",
        "  # Reading the coreference resolution text\n",
        "  with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/New text - Coreference Resolution/\" + title + \" - Coref.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "      coreference_resolved_text.append(line.strip())\n",
        "\n",
        "  for index, sentence in enumerate(coreference_resolved_text):\n",
        "    sentence = contractions.fix(sentence)\n",
        "\n",
        "    flag = 0\n",
        "    outcome = srl_predictor.predict(sentence=sentence)\n",
        "    for item in outcome[\"verbs\"]:\n",
        "\n",
        "      character_text= []\n",
        "      tags = item[\"tags\"]\n",
        "\n",
        "      for j in range(len(tags)):\n",
        "\n",
        "        if tags[j] == \"B-ARG0\":\n",
        "          agent = outcome[\"words\"][j]\n",
        "          flag = 1\n",
        "          continue\n",
        "\n",
        "        while flag == 1:\n",
        "\n",
        "          if tags[j] == \"I-ARG0\":\n",
        "            agent = agent + \" \" + outcome[\"words\"][j]\n",
        "            break\n",
        "\n",
        "          else:\n",
        "\n",
        "            character_text.append(index)\n",
        "            character_text.append(agent)\n",
        "\n",
        "            # Writing the agents and the sentence index into the CSV file\n",
        "            with open('/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Semantic Role Labelling - Coref/' + title + \" - Semantic Role Labelling.csv\", 'a', newline='') as csvf:\n",
        "              csv_w = csv.writer(csvf)\n",
        "              num_elements = len(character_text)\n",
        "              for i in range(0, num_elements, 2):\n",
        "                  row = character_text[i:i + 2]\n",
        "                  csv_w.writerow(row)\n",
        "\n",
        "            flag = 0"
      ],
      "metadata": {
        "id": "hBBOCFr8qLrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. Getting the Character based sentences using the Window size:"
      ],
      "metadata": {
        "id": "jmBjGP09q3-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coreference Resolved Text\n",
        "def character_based_sentences_coref(title):\n",
        "\n",
        "  window_size = 10000\n",
        "  num_of_chuncks = 100\n",
        "  top_10_characters = []\n",
        "  coreference_resolved_text = []\n",
        "\n",
        "  # Reading the semantic role labelled file\n",
        "  srl_column = [\"sentence_index\", \"agent\"]\n",
        "  srl_df = pd.read_csv(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Semantic Role Labelling - Coref/\" + title + \" - Semantic Role Labelling.csv\", usecols = srl_column)\n",
        "\n",
        "  # Reading the top 3 characters\n",
        "  with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Top 10 characters/\" + title + \" - Top 10 Characters.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "      character = line.strip().split(\",\")[0]\n",
        "      top_10_characters.append(character)\n",
        "\n",
        "  top_3_characters = top_10_characters.copy()\n",
        "\n",
        "  # Reading the top 3 characters in each book\n",
        "  if title == \"The Road to Oz\":\n",
        "    top_3_characters = []\n",
        "    top_3_characters.append(top_10_characters[0])\n",
        "    top_3_characters.append(top_10_characters[2])\n",
        "    top_3_characters.append(top_10_characters[3])\n",
        "\n",
        "  elif title == \"The Scarecrow of Oz\":\n",
        "    top_3_characters = top_3_characters[0:2]\n",
        "    top_3_characters.append(top_10_characters[3])\n",
        "\n",
        "  else:\n",
        "    top_3_characters = top_3_characters[0:3]\n",
        "\n",
        "  # Reading the word list along with the index values to get the window size\n",
        "  columns_to_read = [\"words\", \"index_position\", \"sentence_id\"]\n",
        "  window_df = pd.read_csv(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/DataFrame to get the Window - Coref/\" + title + \" - Words.csv\", usecols = columns_to_read, skiprows = [1])\n",
        "\n",
        "  # Reading the Coreference Resolved text for the Character's Arc\n",
        "  with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/New text - Coreference Resolution/\" + title + \" - Coref.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "      coreference_resolved_text.append(line.strip())\n",
        "\n",
        "  step_size = int(math.floor(window_df.shape[0] - window_size) / (num_of_chuncks - 1))\n",
        "\n",
        "  for character in top_3_characters:\n",
        "\n",
        "    for i in range(num_of_chuncks):\n",
        "\n",
        "      character_resolved_text = []\n",
        "      row_to_append = []\n",
        "      character_presence = []\n",
        "      row = []\n",
        "\n",
        "      # Getting the start and end sentence index positions\n",
        "      index_start_word = i * step_size\n",
        "      index_end_word = window_size + (i * step_size)\n",
        "\n",
        "      start_word_row = window_df.loc[(window_df[\"index_position\"] == index_start_word)]\n",
        "      start_id = start_word_row.loc[start_word_row.index[0], \"sentence_id\"]\n",
        "      start_sentence_index = start_id - 1\n",
        "\n",
        "      end_word_row = window_df.loc[(window_df[\"index_position\"] == index_end_word)]\n",
        "      end_id = end_word_row.loc[end_word_row.index[0], \"sentence_id\"]\n",
        "      end_sentence_index = end_id\n",
        "\n",
        "      character_df = srl_df[(srl_df[\"sentence_index\"] >= start_sentence_index) & (srl_df[\"sentence_index\"] <= end_sentence_index) & (srl_df[\"agent\"].str.contains(character_corrected))]\n",
        "      character_text_list = character_df[\"sentence_index\"].tolist()\n",
        "      character_text_list = list(set(character_text_list))\n",
        "      character_text_list_sorted = character_text_list.copy()\n",
        "      character_text_list_sorted.sort()\n",
        "\n",
        "      # Getting the text from the window size\n",
        "      if i != num_of_chuncks - 1:\n",
        "        text = coreference_resolved_text[start_sentence_index:end_sentence_index]\n",
        "\n",
        "      else:\n",
        "        text = coreference_resolved_text[start_sentence_index:]\n",
        "\n",
        "      block_of_text = \" \".join(list(map(str, text)))\n",
        "\n",
        "      # Getting the character related text from the window size\n",
        "      for index in character_text_list_sorted:\n",
        "        character_resolved_text.append(coreference_resolved_text[index])\n",
        "\n",
        "      character_block_text = \" \".join(list(map(str, character_resolved_text)))\n",
        "\n",
        "      # Append it to a list\n",
        "      row_to_append.append(i + 1)\n",
        "      row_to_append.append(block_of_text)\n",
        "      row_to_append.append(len(text))\n",
        "      row_to_append.append(character_block_text)\n",
        "      row_to_append.append(len(character_resolved_text))\n",
        "\n",
        "      # Adding it into a CSV file\n",
        "      with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Character Text for Character Arc Generation - Coref/\" + title + \"/\" + title + \" - \" + character + \".csv\", mode = 'a', newline = '') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(row_to_append)"
      ],
      "metadata": {
        "id": "SxMToTYFrK_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18. Emotion Arc related functions:"
      ],
      "metadata": {
        "id": "vLFbVBCPzNA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the count of each word in the text\n",
        "\n",
        "def word_freq(words_in_text):\n",
        "    count_dict = {}\n",
        "\n",
        "    for word in words_in_text:\n",
        "        if word in count_dict:\n",
        "            count_dict[word] += 1\n",
        "        else:\n",
        "            count_dict[word] = 1\n",
        "\n",
        "    return count_dict"
      ],
      "metadata": {
        "id": "Dhj12tJLzHC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopper Function\n",
        "# This function was taken directly from the Documentation written by the authors\n",
        "# Referencing the author's work as there were issues with accessing this function\n",
        "# Reference: Mitchell L. Kiley D. Danforth C.M. Reagan, A.J. and P.S. Dodds. The emotional arcs of stories are dominated by six basic shapes, 2016.\n",
        "\n",
        "def stopper(tmpVec,score_list,word_list,stopVal=1.0,ignore=[],center=5.0):\n",
        "    \"\"\"Take a frequency vector, and 0 out the stop words.\n",
        "\n",
        "    Will always remove the nig* words.\n",
        "\n",
        "    Return the 0'ed vector.\"\"\"\n",
        "\n",
        "    ignoreWords = [\"nigga\",\"nigger\",\"niggaz\",\"niggas\"];\n",
        "    for word in ignore:\n",
        "        ignoreWords.append(word)\n",
        "    newVec = tmpVec\n",
        "    for i in range(len(score_list)):\n",
        "        if abs(score_list[i]-center) < stopVal:\n",
        "            newVec[i] = 0\n",
        "        if word_list[i] in ignoreWords:\n",
        "            newVec[i] = 0\n",
        "\n",
        "    return newVec"
      ],
      "metadata": {
        "id": "ipndJmEzzmNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 19. Generating the Character Arc of the Top 3 characters in each of the books:"
      ],
      "metadata": {
        "id": "tejcgpSXtTTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an instance of the LabMT class\n",
        "LabMT = LabMT()"
      ],
      "metadata": {
        "id": "QXIIkJZ_FiLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def emotional_arc(title, path1, path2):\n",
        "\n",
        "  num_of_chuncks = 100\n",
        "  top_10_characters = []\n",
        "  word_tokenizer = English()\n",
        "\n",
        "  # Reading the top 3 characters\n",
        "  with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Top 10 characters/\" + title + \" - Top 10 Characters.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "      character = line.strip().split(\",\")[0]\n",
        "      top_10_characters.append(character)\n",
        "\n",
        "  top_3_characters = top_10_characters.copy()\n",
        "\n",
        "  # Reading the top 3 characters in each book\n",
        "  if title == \"The Road to Oz\":\n",
        "    top_3_characters = []\n",
        "    top_3_characters.append(top_10_characters[0])\n",
        "    top_3_characters.append(top_10_characters[2])\n",
        "    top_3_characters.append(top_10_characters[3])\n",
        "\n",
        "  elif title == \"The Scarecrow of Oz\":\n",
        "    top_3_characters = top_3_characters[0:2]\n",
        "    top_3_characters.append(top_10_characters[3])\n",
        "\n",
        "  else:\n",
        "    top_3_characters = top_3_characters[0:3]\n",
        "\n",
        "  for character in top_3_characters:\n",
        "\n",
        "    # Reading the respective file for the character based sentences\n",
        "    character_sentences_dataframe = pd.read_csv(path1 + title + \"/\" + title + \" - \" + character + \".csv\", skiprows = [1])\n",
        "\n",
        "    freq_matrix = lil_matrix((num_of_chuncks, len(LabMT.scorelist)))\n",
        "\n",
        "    for counter, block_of_text in enumerate(character_sentences_dataframe[\"character_text_in_window\"]):\n",
        "\n",
        "      block_of_text = str(block_of_text)\n",
        "\n",
        "      # Word Tokenisation for the block of text\n",
        "      all_words_in_sentences = []\n",
        "      words = word_tokenizer(block_of_text.strip())\n",
        "      for word in words:\n",
        "        if not word.is_punct and len(word) != 0 and word.text.strip() != \"\":\n",
        "          all_words_in_sentences.append(word.text.lower())\n",
        "\n",
        "      word_freq_dict = word_freq(all_words_in_sentences)\n",
        "      freq_matrix[counter, :] = LabMT.wordVecify(word_freq_dict)\n",
        "\n",
        "    # Generating the emotion scores\n",
        "    emotion_score = list(np.zeros((num_of_chuncks)))\n",
        "    X = []\n",
        "\n",
        "    for i in range(num_of_chuncks):\n",
        "        X.append(i)\n",
        "\n",
        "    for i in range(num_of_chuncks):\n",
        "\n",
        "      # Convert each row of the sparse matrix representation into a list\n",
        "      text_vec = freq_matrix[i, :].toarray().squeeze()\n",
        "\n",
        "      stoppedVec = stopper(text_vec,LabMT.scorelist,LabMT.wordlist)\n",
        "\n",
        "      # Getting the scores based on the frequency of the words appearance in the chunck\n",
        "      emotion_score[i] = np.dot(LabMT.scorelist,stoppedVec)/sum(stoppedVec)\n",
        "\n",
        "    emotion_score = [0 if math.isnan(score) else score for score in emotion_score]\n",
        "\n",
        "    # Saving the emotion scores\n",
        "    with open(path2 + title + \"/\" + title + \" - \" + character + \".csv\", \"a\", newline = \"\") as csvf:\n",
        "      csv_w = csv.writer(csvf)\n",
        "\n",
        "      csv_w.writerow([\"window_number\", \"emotion_score\"])\n",
        "\n",
        "      for ele1, ele2 in zip(X, emotion_score):\n",
        "          csv_w.writerow([ele1, ele2])"
      ],
      "metadata": {
        "id": "6AOhPNzdtbNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20. Drawing the Arc's:"
      ],
      "metadata": {
        "id": "Q3AuJCQpfN0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing the character arcs for each book\n",
        "def character_arc_compare(title, path1, path2):\n",
        "\n",
        "  top_10_characters = []\n",
        "\n",
        "  # Reading the top 3 characters\n",
        "  with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Top 10 characters/\" + title + \" - Top 10 Characters.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "      character = line.strip().split(\",\")[0]\n",
        "      top_10_characters.append(character)\n",
        "\n",
        "  top_3_characters = top_10_characters.copy()\n",
        "\n",
        "  # Reading the top 3 characters in each book\n",
        "  if title == \"The Road to Oz\":\n",
        "    top_3_characters = []\n",
        "    top_3_characters.append(top_10_characters[0])\n",
        "    top_3_characters.append(top_10_characters[2])\n",
        "    top_3_characters.append(top_10_characters[3])\n",
        "\n",
        "  elif title == \"The Scarecrow of Oz\":\n",
        "    top_3_characters = top_3_characters[0:2]\n",
        "    top_3_characters.append(top_10_characters[3])\n",
        "\n",
        "  else:\n",
        "    top_3_characters = top_3_characters[0:3]\n",
        "\n",
        "  figure, ax = plt.subplots()\n",
        "\n",
        "  for character in top_3_characters:\n",
        "\n",
        "    X = []\n",
        "    emotional_score = []\n",
        "\n",
        "    with open(path1 + title + \" - \" + character + \".csv\", \"r\") as csvfile:\n",
        "      csvreader = csv.reader(csvfile)\n",
        "      for row in csvreader:\n",
        "        if len(row) >= 2:\n",
        "          X.append(row[0])\n",
        "          emotional_score.append(row[1])\n",
        "\n",
        "    X = X[1:]\n",
        "    emotional_score = emotional_score[1:]\n",
        "    X = [int(x) for x in X]\n",
        "    emotional_score = [float(x) for x in emotional_score]\n",
        "    X = [x for x, score in zip(X, emotional_score) if score != 0]\n",
        "    emotional_score = [score for score in emotional_score if score != 0]\n",
        "\n",
        "    ax.plot(X, emotional_score, label = character)\n",
        "    ax.legend()\n",
        "\n",
        "  ax.set_xlabel(\"Percentage of Book\")\n",
        "  ax.set_ylabel(\"Emotional Score based on Valence\")\n",
        "  # ax.set_title(title, fontsize = 18)\n",
        "  #plt.savefig(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Results - Plots/Character Arc Comparison/\" + title + \".pdf\")\n",
        "  plt.show()\n",
        "  print()\n",
        "  print()"
      ],
      "metadata": {
        "id": "Ws8_W7YHBSJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the individual character arcs\n",
        "def individual_arcs(path, title):\n",
        "\n",
        "  if title == \"The Road to Oz\":\n",
        "    top_10_characters = []\n",
        "\n",
        "    # Reading the top 3 characters\n",
        "    with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Top 10 characters/\" + title + \" - Top 10 Characters.txt\", 'r') as f:\n",
        "      for line in f:\n",
        "        character = line.strip().split(\",\")[0]\n",
        "        top_10_characters.append(character)\n",
        "\n",
        "    top_3_characters = top_10_characters.copy()\n",
        "\n",
        "    # Reading the top 3 characters in each book\n",
        "    if title == \"The Road to Oz\":\n",
        "      top_3_characters = []\n",
        "      top_3_characters.append(top_10_characters[0])\n",
        "      top_3_characters.append(top_10_characters[2])\n",
        "      top_3_characters.append(top_10_characters[3])\n",
        "\n",
        "    elif title == \"The Scarecrow of Oz\":\n",
        "      top_3_characters = top_3_characters[0:2]\n",
        "      top_3_characters.append(top_10_characters[3])\n",
        "\n",
        "    else:\n",
        "      top_3_characters = top_3_characters[0:3]\n",
        "\n",
        "    for character in top_3_characters:\n",
        "\n",
        "      X = []\n",
        "      emotional_score = []\n",
        "      new_path = path + title + \" - \" + character + \".csv\"\n",
        "\n",
        "      with open(new_path, \"r\") as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "        for row in csvreader:\n",
        "          if len(row) >= 2:\n",
        "            X.append(row[0])\n",
        "            emotional_score.append(row[1])\n",
        "\n",
        "      X = X[1:]\n",
        "      emotional_score = emotional_score[1:]\n",
        "      X = [int(x) for x in X]\n",
        "      emotional_score = [float(x) for x in emotional_score]\n",
        "      X = [x for x, score in zip(X, emotional_score) if score != 0]\n",
        "      emotional_score = [score for score in emotional_score if score != 0]\n",
        "\n",
        "      plt.plot(X, emotional_score)\n",
        "      plt.xlabel(\"Percentage of the Book\")\n",
        "      plt.ylabel(\"Emotional Score based on Valence\")\n",
        "      plt.title(title + \" - \" + character)\n",
        "      plt.savefig(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Results - Plots/Character Arcs/\" + title + \"/\" + title + \" - \" + character + \" - Final.pdf\")\n",
        "      plt.show()\n",
        "      print()\n",
        "      print()\n"
      ],
      "metadata": {
        "id": "TWYO7pUjCm1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing individual character arcs with the emotion arc of the narrative\n",
        "def character_and_emotional_comparison_individual(title, path1, path2):\n",
        "\n",
        "  top_10_characters = []\n",
        "  X1 = []\n",
        "  emotional_arc = []\n",
        "\n",
        "  # Reading the top 3 characters\n",
        "  with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Top 10 characters/\" + title + \" - Top 10 Characters.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "      character = line.strip().split(\",\")[0]\n",
        "      top_10_characters.append(character)\n",
        "\n",
        "  top_3_characters = top_10_characters.copy()\n",
        "\n",
        "  # Reading the top 3 characters in each book\n",
        "  if title == \"The Road to Oz\":\n",
        "    top_3_characters = []\n",
        "    top_3_characters.append(top_10_characters[0])\n",
        "    top_3_characters.append(top_10_characters[2])\n",
        "    top_3_characters.append(top_10_characters[3])\n",
        "\n",
        "  elif title == \"The Scarecrow of Oz\":\n",
        "    top_3_characters = top_3_characters[0:2]\n",
        "    top_3_characters.append(top_10_characters[3])\n",
        "\n",
        "  else:\n",
        "    top_3_characters = top_3_characters[0:3]\n",
        "\n",
        "  # Reading the Emotion Arc\n",
        "  with open(path2, \"r\") as csvfile:\n",
        "    csvreader = csv.reader(csvfile)\n",
        "    for row in csvreader:\n",
        "      if len(row) >= 2:\n",
        "        X1.append(row[0])\n",
        "        emotional_arc.append(row[1])\n",
        "\n",
        "  X1 = X1[1:]\n",
        "  emotional_arc = emotional_arc[1:]\n",
        "  X1 = [int(x) for x in X1]\n",
        "  emotional_arc = [float(x) for x in emotional_arc]\n",
        "  X1 = [x for x, score in zip(X1, emotional_arc) if score != 0]\n",
        "  emotional_arc = [score for score in emotional_arc if score != 0]\n",
        "\n",
        "  for character in top_3_characters:\n",
        "\n",
        "    # Reading the Character Arc\n",
        "    filename = path1 + title + \" - \" + character + \".csv\"\n",
        "    character_score = []\n",
        "    X = []\n",
        "\n",
        "    with open(filename, \"r\") as csvfile:\n",
        "      csvreader = csv.reader(csvfile)\n",
        "      for row in csvreader:\n",
        "        if len(row) >= 2:\n",
        "          X.append(row[0])\n",
        "          character_score.append(row[1])\n",
        "\n",
        "    X = X[1:]\n",
        "    character_score = character_score[1:]\n",
        "    X = [int(x) for x in X]\n",
        "    character_score = [float(x) for x in character_score]\n",
        "    X = [x for x, score in zip(X, character_score) if score != 0]\n",
        "    character_score = [score for score in character_score if score != 0]\n",
        "\n",
        "    fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 5))\n",
        "\n",
        "    axes[0].plot(X, character_score)\n",
        "    axes[0].set_xlabel(\"Percentage of Book\")\n",
        "    axes[0].set_ylabel(\"Emotional Score based on Valence\")\n",
        "    axes[0].set_title(\"Character Arc - \" + character)\n",
        "\n",
        "    axes[1].plot(X1, emotional_arc)\n",
        "    axes[1].set_xlabel(\"Percentage of Book\")\n",
        "    axes[1].set_ylabel(\"Emotional Score based on Valence\")\n",
        "    axes[1].set_title(\"Emotion Arc of \" + title)\n",
        "\n",
        "    fig.suptitle(title + \" - \" + character, fontsize = 32)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Results - Plots/Character and Emotion Arc Comparison/\" + title + \"/\" + title + \" - \" + character + \".pdf\")\n",
        "    plt.show()\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "id": "SJ0JacQWOgCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 21. Main Function:"
      ],
      "metadata": {
        "id": "5LkwIertj7TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/The Wizard of Oz\"\n",
        "\n",
        "# Getting all the books in the Folder\n",
        "for root, _, files in os.walk(path):\n",
        "    for file in files:\n",
        "        if file != \".DS_Store\":\n",
        "\n",
        "          b = os.path.join(root, file)\n",
        "\n",
        "          # Opening the books using codecs\n",
        "          book = codecs.open(b, \"r\", encoding = \"utf-8\")\n",
        "\n",
        "          # Getting the Title of the book\n",
        "          title = get_title(book)\n",
        "\n",
        "          # Removing the uneccesary information related to Project Gutenberg\n",
        "          # at the beginning and end of the book\n",
        "          book = codecs.open(b, \"r\", encoding = \"utf-8\")\n",
        "          book_clean = clean_end_first(book, file)\n",
        "\n",
        "          # Getting all the chapter titles from the book in order\n",
        "          chapter_title = chapter_titles(book_clean, file)\n",
        "\n",
        "          # Getting the text associated with each chapter in order\n",
        "          chapter_chuncks = chapter_chunck(chapter_title, book_clean, file)\n",
        "\n",
        "          # Removing paragraph spacing and line breaks\n",
        "          chapter_chuncks_clean = remove_spacing_line_breaks(chapter_chuncks)\n",
        "\n",
        "          # Sentence Tokenisation for each chapter in book\n",
        "          all_sents_in_each_chapter = sent_tokenise(chapter_chuncks_clean)\n",
        "\n",
        "          # Getting all the words in the whole text\n",
        "          all_sents_list = list(itertools.chain(*all_sents_in_each_chapter))\n",
        "\n",
        "          # Named Entity Recognition using AllenNLP and the results will be stored in a CSV file\n",
        "          # Characters name and the sentence ID\n",
        "          PER_tags = NER(all_sents_list, title)\n",
        "\n",
        "          # Reading the Named Entity Recognition CSV file\n",
        "          # List of all appearances of the name\n",
        "          list_of_names = NER_read_csv(title)\n",
        "\n",
        "          # Making corrections to some of the NER model tags\n",
        "          list_of_names_corrected = correction_function(list_of_names, title)\n",
        "\n",
        "          # Using the NameParser to get the titles of all the characters in the novel\n",
        "          # This includes the following information: title, firstname, middlename, lastname, suffix and nickname\n",
        "          name_info = get_name_info(list_of_names_corrected)\n",
        "\n",
        "          # Getting all the information together in a single list\n",
        "          total_names_info = []\n",
        "\n",
        "          for i in range(len(list_of_names_corrected)):\n",
        "            d1 = list_of_names_corrected[i]\n",
        "            d2 = name_info[i]\n",
        "            d3 = {\"gender\": \"\"}\n",
        "\n",
        "            d4 = {**d1, **d2, **d3}\n",
        "            total_names_info.append(d4)\n",
        "\n",
        "          # Getting the gender of the characters\n",
        "          total_names_info = gender_identification(total_names_info, title)\n",
        "\n",
        "          # Removing the characters with no gender or unknown genders\n",
        "          total_names_info_corrected = []\n",
        "\n",
        "          for i in range(len(total_names_info)):\n",
        "            if total_names_info[i][\"gender\"] != \"unknown\" and total_names_info[i][\"gender\"] != \"\":\n",
        "              total_names_info_corrected.append(total_names_info[i])\n",
        "\n",
        "          # Storing all the information in a CSV file\n",
        "          store_all_info(total_names_info_corrected, title)\n",
        "\n",
        "          # Getting the characters in the book\n",
        "          characters = get_characters(total_names_info_corrected)\n",
        "\n",
        "          # Saving the characters in the novel in a text file\n",
        "          with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Characters/\" + title + \" - Characters.txt\", 'w') as f:\n",
        "            for character in characters:\n",
        "              f.write(str(character) + '\\n')\n",
        "\n",
        "          # Matching Algorithm\n",
        "          all_entities = matching_algorithm(title)\n",
        "\n",
        "          # Reading nickname information from the text file\n",
        "          unique_entities = nicknames_and_matches(title, all_entities)\n",
        "\n",
        "          # Storing the matching entities in a text file\n",
        "          with open(\"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Matched Characters and Nicknames/\" + title + \" - Matches.txt\", 'w') as f:\n",
        "            for i, entity in enumerate(unique_entities):\n",
        "              line = ','.join(entity) + '\\n'\n",
        "              f.write(line)\n",
        "\n",
        "          # Entity Resolution and getting the top 10 characters based on their occurence\n",
        "          entity_resolution(title)\n",
        "          top_10_characters(title)\n",
        "          resolution(title, all_sents_list)\n",
        "\n",
        "          # Coreference Resolution\n",
        "          coreference_resolution(title)\n",
        "\n",
        "          # Semantic Role Labelling\n",
        "          semantic_role_labelling_coref(title)\n",
        "\n",
        "          character_based_sentences_coref(title)\n",
        "\n",
        "          # Character Arc Generation\n",
        "          emotional_arc(title, \"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Character Text for Character Arc Generation - Coref/\", \"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Character Arc - Coref/\")\n",
        "\n",
        "          # Comparing the character arcs for each book\n",
        "          path5 = \"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Character Arc - Coref/\" + title + \"/\"\n",
        "          path6 = \"/content/drive/MyDrive/The Wizard of Oz/Emotional Arc - Trial 1/\" + title + \"/\" + title + \".csv\"\n",
        "          character_arc_compare(title, path5, path6)\n",
        "\n",
        "          # Plotting the individual character arcs\n",
        "          path7 = \"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Character Arc - Coref/\" + title + \"/\"\n",
        "          individual_arcs(path7, title)\n",
        "\n",
        "          # Comparing individual character arcs with the emotion arc of the narrative\n",
        "          path8 = \"/content/drive/MyDrive/The Wizard of Oz/Character Arc - Trial 1/Final Outcomes/Character Arc - Coref/\" + title + \"/\"\n",
        "          path9 = \"/content/drive/MyDrive/The Wizard of Oz/Emotional Arc - Trial 1/\" + title + \"/\" + title + \".csv\"\n",
        "          character_and_emotional_comparison_individual(title, path8, path9)"
      ],
      "metadata": {
        "id": "HzCA4RamvzX1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}