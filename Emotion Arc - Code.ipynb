{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "95f60780",
      "metadata": {
        "id": "95f60780"
      },
      "source": [
        "# 1. Import Statements:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "id": "vueY6ooIeTfK"
      },
      "id": "vueY6ooIeTfK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install labMTsimple"
      ],
      "metadata": {
        "id": "IBF9AXBKeh3O"
      },
      "id": "IBF9AXBKeh3O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install marisa-trie"
      ],
      "metadata": {
        "id": "cFgVy3ENeu1-"
      },
      "id": "cFgVy3ENeu1-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install word2number"
      ],
      "metadata": {
        "id": "BiJY6sj2e3jT"
      },
      "id": "BiJY6sj2e3jT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nameparser"
      ],
      "metadata": {
        "id": "NW-TEKveLgXn"
      },
      "id": "NW-TEKveLgXn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d662379",
      "metadata": {
        "id": "3d662379"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import math\n",
        "import numpy as np\n",
        "import codecs\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import contractions\n",
        "import itertools\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from scipy.sparse import lil_matrix\n",
        "from labMTsimple.speedy import LabMT\n",
        "from word2number import w2n\n",
        "from nameparser import HumanName\n",
        "import nltk\n",
        "from nltk.corpus import names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nmmYYW0pd01c"
      },
      "id": "nmmYYW0pd01c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "958b8a1d",
      "metadata": {
        "id": "958b8a1d"
      },
      "outputs": [],
      "source": [
        "# Creating an instance of the LabMT class\n",
        "LabMT = LabMT()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12cca666",
      "metadata": {
        "id": "12cca666"
      },
      "source": [
        "# 1. Function to get the title of the book:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b340ba",
      "metadata": {
        "id": "46b340ba"
      },
      "outputs": [],
      "source": [
        "def get_title(book):\n",
        "\n",
        "    # Getting the Title of the book\n",
        "    pattern1 = \"Title:\\s(.*)\"\n",
        "    match = re.search(pattern1, book.read())\n",
        "    title = match.group(1)\n",
        "\n",
        "    return title.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d622ce",
      "metadata": {
        "id": "87d622ce"
      },
      "source": [
        "# 2. Function to remove the first and last part of the book (Unnecessary):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd0aaf3",
      "metadata": {
        "id": "5cd0aaf3"
      },
      "outputs": [],
      "source": [
        "def clean_end_first(book, filename):\n",
        "\n",
        "    # Removing the first part of the book\n",
        "    pattern2 = \"\\*\\*\\*\\s?START.*?\\s?\\*\\*\\*\"\n",
        "    book_clean = re.sub(\"^(?s:.*?)\" + pattern2, \" \", book.read(), count = 1)\n",
        "\n",
        "    # Removing the last part of the book\n",
        "    pattern3 = \"\\*\\*\\*\\s?END.*?\\s?\\*\\*\\*\"\n",
        "    book_clean = re.sub(pattern3 + \"(?s:.*)\", \" \", book_clean, count = 1)\n",
        "\n",
        "    # Removing [Illustrations: ...] from the books\n",
        "    pattern4 = \"\\[Illustration:(.*)\\]\"\n",
        "    book_clean = re.sub(pattern4, \"\", book_clean)\n",
        "\n",
        "    # Removing the text \"The End\" and others from the book\n",
        "    pattern5 = \"(?i)the end|end of the project gutenberg ebook of(.*)|end of project gutenberg's(.*)\"\n",
        "    book_clean = re.sub(pattern5, \"\", book_clean)\n",
        "\n",
        "    # On Manual inspection, these books had the list of all books in the Wizard of Oz Series\n",
        "    # This Regex will be used to remove that\n",
        "    if filename == \"8. Tik-Tok of Oz.txt\" or filename == \"9. The Scarecrow of Oz.txt\" or filename == \"10. Rinkitink in Oz.txt\" or filename == \"12. The Tin Woodman of Oz.txt\" or filename == \"14. Glinda of Oz.txt\":\n",
        "        pattern6 = \"(The Wonderful Oz Books|THE FAMOUS OZ BOOKS)\"\n",
        "        book_clean = re.sub(pattern6 + \"(?s:.*)\", \" \", book_clean)\n",
        "\n",
        "    return book_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d3ab370",
      "metadata": {
        "id": "4d3ab370"
      },
      "source": [
        "# 3. Function to Find all the Chapters Titles in the book:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd86abe",
      "metadata": {
        "id": "8dd86abe"
      },
      "outputs": [],
      "source": [
        "def chapter_titles(book, filename):\n",
        "    # Finding all the chapter names in the book\n",
        "    dictTOC = {}\n",
        "    chapter_title = []\n",
        "\n",
        "    if filename == \"3. Ozma of Oz.txt\" or filename == \"4. Dorothy and the Wizard in Oz.txt\" or filename == \"5. The Road to Oz.txt\" or filename == \"13. The Magic of Oz.txt\":\n",
        "        # Pattern seen in Book 3, 4, 5, 13\n",
        "        pattern7 = re.findall(\"(\\d+)(\\.)(\\s+)(.*)\", book)\n",
        "\n",
        "        for i in range(len(pattern7)):\n",
        "            title = pattern7[i][0] + pattern7[i][1] + pattern7[i][2] + pattern7[i][3]\n",
        "            if len(pattern7[i][0]) <= 2:\n",
        "                dictTOC[pattern7[i][0]] = title.strip()\n",
        "    else:\n",
        "        pattern8 = re.findall(\"(Chapter)(.*)\", book)\n",
        "\n",
        "        for i in range(len(pattern8)):\n",
        "            title = pattern8[i][0] + pattern8[i][1]\n",
        "            key_val = w2n.word_to_num(pattern8[i][1])\n",
        "            dictTOC[key_val] = title.strip()\n",
        "\n",
        "    for key in dictTOC:\n",
        "        number = int(key) - 1\n",
        "        chapter_title.insert(number, dictTOC[key])\n",
        "\n",
        "    return chapter_title"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee60f946",
      "metadata": {
        "id": "ee60f946"
      },
      "source": [
        "# 4. Getting the text for each chapter in the book:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7f8c5e0",
      "metadata": {
        "id": "d7f8c5e0"
      },
      "outputs": [],
      "source": [
        "def matching_chapter_titles(chapter_title, book_clean, filename, val):\n",
        "\n",
        "    matches = re.finditer(re.escape(chapter_title), book_clean)\n",
        "    count = 0\n",
        "    temp = list(matches)\n",
        "    length = len(temp)\n",
        "\n",
        "    for i in range(length):\n",
        "        if length == 2:\n",
        "            match_index = temp[1].span()[val]\n",
        "        else:\n",
        "            match_index = temp[0].span()[val]\n",
        "\n",
        "    return match_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62481688",
      "metadata": {
        "id": "62481688"
      },
      "outputs": [],
      "source": [
        "def chapter_chunck(chapter_titles, book_clean, filename):\n",
        "\n",
        "    chapter_chuncks = []\n",
        "\n",
        "    for i in range(len(chapter_titles)):\n",
        "\n",
        "        # Read till the next chapter title\n",
        "        if i != (len(chapter_titles) - 1):\n",
        "\n",
        "            # Getting the chapter title to start and end with\n",
        "            chapter_title = chapter_titles[i]\n",
        "            next_chapter_title = chapter_titles[i + 1]\n",
        "\n",
        "            # Finding the positions in the text\n",
        "            # This will get the chapter chunck\n",
        "\n",
        "            # Start position\n",
        "            end_of_start_chapter_title_index = matching_chapter_titles(chapter_title, book_clean, filename, 1)\n",
        "\n",
        "            # End Position\n",
        "            start_of_next_chapter_title_index = matching_chapter_titles(next_chapter_title, book_clean, filename, 0)\n",
        "\n",
        "            # Getting the chapter chuncks and putting it into a list\n",
        "            chapter_chuncks.append(book_clean[end_of_start_chapter_title_index:start_of_next_chapter_title_index])\n",
        "\n",
        "        # Read till the end\n",
        "        else:\n",
        "            # This is done for the last chapter, where we will read till the end of the book\n",
        "\n",
        "            chapter_title = chapter_titles[i]\n",
        "            end_of_start_chapter_title_index = matching_chapter_titles(chapter_title, book_clean, filename, 1)\n",
        "            chapter_chuncks.append(book_clean[end_of_start_chapter_title_index:])\n",
        "\n",
        "    return chapter_chuncks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e4a9443",
      "metadata": {
        "id": "7e4a9443"
      },
      "source": [
        "# 5. Function to remove paragraph spacing and weird line breaks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71a9fdee",
      "metadata": {
        "id": "71a9fdee"
      },
      "outputs": [],
      "source": [
        "def remove_spacing_line_breaks(chapter_chuncks):\n",
        "\n",
        "    # Each paragraph is separated using \\r\\n\\r\\n\n",
        "    # Each new line is created using \\r\\n\n",
        "    # Both of these will now be replaced using \\s\n",
        "    # Dealing with contractions as well\n",
        "\n",
        "    chapter_chuncks_clean = []\n",
        "\n",
        "    for chapter in chapter_chuncks:\n",
        "        chapter = chapter.strip()\n",
        "        chapter = chapter.replace(\"\\r\\n\", \" \")\n",
        "        chapter = chapter.replace(\"\\r\\n\\r\\n\", \" \")\n",
        "        chapter = contractions.fix(chapter)\n",
        "        chapter_chuncks_clean.append(chapter)\n",
        "\n",
        "    return chapter_chuncks_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c151f120",
      "metadata": {
        "id": "c151f120"
      },
      "source": [
        "# 6. Function to perform Sentence Tokenisation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8486fd9",
      "metadata": {
        "id": "a8486fd9"
      },
      "outputs": [],
      "source": [
        "def sent_tokenise(chapter_chuncks_clean):\n",
        "    # Using SpaCy to perform sentence Tokenization\n",
        "\n",
        "    sent_tokenizer = spacy.load(\"en_core_web_sm\")\n",
        "    all_sents_in_each_chapter = []\n",
        "\n",
        "    for chapter in chapter_chuncks_clean:\n",
        "        sent_chapter = []\n",
        "        sentences = sent_tokenizer(chapter)\n",
        "\n",
        "        for sent in sentences.sents:\n",
        "            sent_chapter.append(sent)\n",
        "\n",
        "        all_sents_in_each_chapter.append(sent_chapter)\n",
        "\n",
        "    return all_sents_in_each_chapter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca367605",
      "metadata": {
        "id": "ca367605"
      },
      "source": [
        "# 7. Function to perform Word Tokenisation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "680b1990",
      "metadata": {
        "id": "680b1990"
      },
      "outputs": [],
      "source": [
        "def word_tokenise(chapter_chuncks_clean):\n",
        "    # Using SpaCy to perform word Tokenization\n",
        "    # Loading the English Tokenizer\n",
        "\n",
        "    word_tokenizer = English()\n",
        "    all_words_in_each_chapter = []\n",
        "\n",
        "    for chapter in chapter_chuncks_clean:\n",
        "        words_chapter = []\n",
        "        words = word_tokenizer(chapter.strip())\n",
        "        for word in words:\n",
        "            if not word.is_punct and len(word) != 0 and word.text.strip() != \"\":\n",
        "                words_chapter.append(word.text.lower())\n",
        "        all_words_in_each_chapter.append(words_chapter)\n",
        "\n",
        "    return all_words_in_each_chapter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1664ee78",
      "metadata": {
        "id": "1664ee78"
      },
      "source": [
        "# 12. Function to create a dictionary with the frequency of each word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9bd3748",
      "metadata": {
        "id": "d9bd3748"
      },
      "outputs": [],
      "source": [
        "# Function to get the count of each word in the text\n",
        "\n",
        "def word_freq(words_in_text):\n",
        "    count_dict = {}\n",
        "\n",
        "    for word in words_in_text:\n",
        "        if word in count_dict:\n",
        "            count_dict[word] += 1\n",
        "        else:\n",
        "            count_dict[word] = 1\n",
        "\n",
        "    return count_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aca8c3cd",
      "metadata": {
        "id": "aca8c3cd"
      },
      "source": [
        "# 13. Stopper Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a999fef0",
      "metadata": {
        "id": "a999fef0"
      },
      "outputs": [],
      "source": [
        "# This function was taken directly from the Documentation written by the authors\n",
        "# Referencing the author's work as there were issues with accessing this function\n",
        "# Reference: Mitchell L. Kiley D. Danforth C.M. Reagan, A.J. and P.S. Dodds. The emotional arcs of stories are dominated by six basic shapes, 2016.\n",
        "\n",
        "def stopper(tmpVec,score_list,word_list,stopVal=1.0,ignore=[],center=5.0):\n",
        "    \"\"\"Take a frequency vector, and 0 out the stop words.\n",
        "\n",
        "    Will always remove the nig* words.\n",
        "\n",
        "    Return the 0'ed vector.\"\"\"\n",
        "\n",
        "    ignoreWords = [\"nigga\",\"nigger\",\"niggaz\",\"niggas\"];\n",
        "    for word in ignore:\n",
        "        ignoreWords.append(word)\n",
        "    newVec = tmpVec\n",
        "    for i in range(len(score_list)):\n",
        "        if abs(score_list[i]-center) < stopVal:\n",
        "            newVec[i] = 0\n",
        "        if word_list[i] in ignoreWords:\n",
        "            newVec[i] = 0\n",
        "\n",
        "    return newVec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08a113e0",
      "metadata": {
        "id": "08a113e0"
      },
      "source": [
        "# 14. Function to Generate the Emotion Arc:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22ec6444",
      "metadata": {
        "id": "22ec6444"
      },
      "outputs": [],
      "source": [
        "def generate_emotional_arc(all_word_list, LabMT, window_size, num_of_chuncks, title, length):\n",
        "\n",
        "    step_size = int(math.floor((len(all_word_list) - window_size) / (num_of_chuncks - 1)))\n",
        "    # Creating a frequency matrix for each chunck of text (100)\n",
        "\n",
        "    # Along the row will be the number of chuncks of text\n",
        "    # Along the columns will be the words in the dictionary (10222 in total)\n",
        "    # We will create a sparse frequency matrix\n",
        "\n",
        "    freq_matrix = lil_matrix((num_of_chuncks, len(LabMT.scorelist)))\n",
        "\n",
        "    for i in range(num_of_chuncks):\n",
        "\n",
        "        if i != num_of_chuncks - 1:\n",
        "\n",
        "            # Get the word frequency dictionary for each chunck of text\n",
        "            word_freq_dict = word_freq(all_word_list[(i * step_size):(window_size + (i * step_size))])\n",
        "\n",
        "            # Converting the frequency matrix into a sparse representation using wordVecify\n",
        "            freq_matrix[i, :] = LabMT.wordVecify(word_freq_dict)\n",
        "\n",
        "        else:\n",
        "            word_freq_dict = word_freq(all_word_list[(i * step_size):])\n",
        "            freq_matrix[i, :] = LabMT.wordVecify(word_freq_dict)\n",
        "\n",
        "    # Generating the emotion Arcs\n",
        "\n",
        "    emotion_score = list(np.zeros((num_of_chuncks)))\n",
        "    X = []\n",
        "\n",
        "    for i in range(num_of_chuncks):\n",
        "        X.append(i)\n",
        "\n",
        "    for i in range(num_of_chuncks):\n",
        "\n",
        "        # Convert each row of the sparse matrix representation into a list\n",
        "        text_vec = freq_matrix[i, :].toarray().squeeze()\n",
        "\n",
        "        stoppedVec = stopper(text_vec,LabMT.scorelist,LabMT.wordlist)\n",
        "\n",
        "        # Getting the scores based on the frequency of the words appearance in the chunck\n",
        "        emotion_score[i] = np.dot(LabMT.scorelist,stoppedVec)/sum(stoppedVec)\n",
        "\n",
        "    # Saving the emotion scores\n",
        "    with open(\"/content/drive/MyDrive/The Wizard of Oz/Emotional Arc - Trial 1/\" + title + \".csv\", \"a\", newline = \"\") as csvf:\n",
        "      csv_w = csv.writer(csvf)\n",
        "\n",
        "      csv_w.writerow([\"window_number\", \"emotion_score\"])\n",
        "\n",
        "      for element1, element2 in zip(X, emotion_score):\n",
        "          csv_w.writerow([element1, element2])\n",
        "\n",
        "    # Plotting the Emotion Arc\n",
        "    plt.plot(X, emotion_score)\n",
        "    plt.title(title + \" - \" + str(length) + \" words\")\n",
        "    plt.xlabel(\"Percentage of Book\")\n",
        "    plt.ylabel(\"Emotional Score based on Valence\")\n",
        "\n",
        "    # Displaying the Emotion Arc\n",
        "    # plt.savefig(\"/content/drive/MyDrive/The Wizard of Oz/Emotional Arc - Trial 1/\" + title + \".pdf\")\n",
        "    plt.show()\n",
        "    print()\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. Plotting the Predicted and Gold Standard Emotion Arcs:"
      ],
      "metadata": {
        "id": "d8p394mSFoO-"
      },
      "id": "d8p394mSFoO-"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(title, file):\n",
        "\n",
        "  path = \"/content/drive/MyDrive/The Wizard of Oz/Emotional Arc - Trial 1/\" + title + \"/\" + title\n",
        "  X1 = []\n",
        "  X2 = []\n",
        "  emotional_score1 = []\n",
        "  emotional_score2 = []\n",
        "\n",
        "  # Reading the Generated Score\n",
        "  with open(path + \".csv\", \"r\") as csvf:\n",
        "    csvr = csv.reader(csvf)\n",
        "    for row in csvr:\n",
        "      if len(row) >= 2:\n",
        "        X1.append(row[0])\n",
        "        emotional_score1.append(row[1])\n",
        "  X1 = X1[1:]\n",
        "  emotional_score1 = emotional_score1[1:]\n",
        "  X1 = [int(x) for x in X1]\n",
        "  emotional_score1 = [float(x) for x in emotional_score1]\n",
        "  X1 = [x for x, score in zip(X1, emotional_score1) if score != 0]\n",
        "  emotional_score1 = [score for score in emotional_score1 if score != 0]\n",
        "\n",
        "  # Reading the Evaluation Score\n",
        "  with open(path + \" - Gold.csv\", \"r\") as csvf:\n",
        "    csvr = csv.reader(csvf)\n",
        "    for row in csvr:\n",
        "      if len(row) >= 2:\n",
        "        X2.append(row[0])\n",
        "        emotional_score2.append(row[1])\n",
        "  X2 = X2[1:]\n",
        "  emotional_score2 = emotional_score2[1:]\n",
        "  X2 = [int(x) for x in X2]\n",
        "  emotional_score2 = [float(x) for x in emotional_score2]\n",
        "  X2 = [x for x, score in zip(X2, emotional_score2) if score != 0]\n",
        "  emotional_score2 = [score for score in emotional_score2 if score != 0]\n",
        "\n",
        "  # Calculating and saving the mean squared error\n",
        "  error = []\n",
        "  for i in range(len(emotional_score2)):\n",
        "    temp = (emotional_score2[i] - emotional_score1[i]) ** 2\n",
        "    error.append(temp)\n",
        "  mean_sqaured_error = sum(error) / len(emotional_score2)\n",
        "\n",
        "  # Saving the Mean Squared Error Values\n",
        "  text = \"MSE of \" + title + \": \" + str(mean_sqaured_error)\n",
        "  with open(\"/content/drive/MyDrive/The Wizard of Oz/Emotional Arc - Trial 1/Mean Sqaured Error Values.txt\", \"a\") as f:\n",
        "    f.write(text)\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "  # Generating the plots\n",
        "  figure, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 5))\n",
        "\n",
        "  axes[0].plot(X1, emotional_score1)\n",
        "  axes[0].set_xlabel(\"Percentage of Book\")\n",
        "  axes[0].set_ylabel(\"Emotional Score based on Valence\")\n",
        "  axes[0].set_title(\"Generated Emotion Arc\")\n",
        "\n",
        "  axes[1].plot(X2, emotional_score2)\n",
        "  axes[1].set_xlabel(\"Percentage of Book\")\n",
        "  axes[1].set_ylabel(\"Emotional Score based on Valence\")\n",
        "  axes[1].set_title(\"Gold Standard Emotion Arc\")\n",
        "\n",
        "  figure.suptitle(title, fontsize = 32)\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(\"/content/drive/MyDrive/The Wizard of Oz/Emotional Arc - Trial 1/\" + title + \"/\" + title + \" - Comparison.pdf\")\n",
        "  plt.show()\n",
        "  print()\n",
        "  print()"
      ],
      "metadata": {
        "id": "hl0BXPtaFn-U"
      },
      "id": "hl0BXPtaFn-U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. Main Function:"
      ],
      "metadata": {
        "id": "tpUgtC8zNbtw"
      },
      "id": "tpUgtC8zNbtw"
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/The Wizard of Oz\"\n",
        "counter = 0\n",
        "\n",
        "# Getting all the books in the Folder\n",
        "for root, _, files in os.walk(path):\n",
        "    for file in files:\n",
        "        if file != \".DS_Store\":\n",
        "\n",
        "          b = os.path.join(root, file)\n",
        "          # Opening the books using codecs\n",
        "          book = codecs.open(b, \"r\", encoding = \"utf-8\")\n",
        "\n",
        "          # Getting the Title of the book\n",
        "          title = get_title(book)\n",
        "\n",
        "          # Removing the uneccesary information related to Project Gutenberg\n",
        "          # at the beginning and end of the book\n",
        "          book = codecs.open(b, \"r\", encoding = \"utf-8\")\n",
        "          book_clean = clean_end_first(book, file)\n",
        "\n",
        "          # Getting all the chapter titles from the book in order\n",
        "          chapter_title = chapter_titles(book_clean, file)\n",
        "\n",
        "          # Getting the text associated with each chapter in order\n",
        "          chapter_chuncks = chapter_chunck(chapter_title, book_clean, file)\n",
        "\n",
        "          # Removing paragraph spacing and line breaks\n",
        "          chapter_chuncks_clean = remove_spacing_line_breaks(chapter_chuncks)\n",
        "\n",
        "          # Sentence Tokenisation for each chapter in book\n",
        "          all_sents_in_each_chapter = sent_tokenise(chapter_chuncks_clean)\n",
        "\n",
        "          # Getting all the words in the whole text\n",
        "          all_sents_list = list(itertools.chain(*all_sents_in_each_chapter))\n",
        "\n",
        "          # Word Tokenisation for each chapter in book\n",
        "          all_words_in_each_chapter = word_tokenise(chapter_chuncks_clean)\n",
        "\n",
        "          # Getting all the words in the whole text\n",
        "          all_word_list = list(itertools.chain(*all_words_in_each_chapter))\n",
        "\n",
        "          # The number of words in the book\n",
        "          length_of_book = len(all_word_list)\n",
        "\n",
        "          # Emotion Arc Generation\n",
        "          emotion_score = generate_emotional_arc(all_word_list, LabMT, 10000, 100, title, length_of_book)\n",
        "\n",
        "          # Evaluating the Emotion Arcs against the Gold Standard Emotion Arcs\n",
        "          evaluate(title, file)"
      ],
      "metadata": {
        "id": "XZwsMfR7xgFd"
      },
      "id": "XZwsMfR7xgFd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}